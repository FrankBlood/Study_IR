<meta http-equiv="content-type" content="text/html; charset=UTF-8">
# 18. A Self-Attentive Sentence Embedding
**一个自注意力的句子嵌入表达 by Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou & Yoshua Bengio for ICLR 2017**

## 摘要
* **本文内容：**提出一个新的模型——通过自注意力提取一个可理解的句子嵌入表达
* 特点：用2维矩阵来表达句子而不是1维向量，其中矩阵的每一行表示句子的不同方面的特征
* 创新：模型中使用自注意力机制和一个特殊的正则项
* 评价：在三个不同的任务中进行评测——author profiling / sentiment classification / textual entailment
* 结果：结果很好哟

## 简介
**本部分主要讲了从词到段落嵌入式表达的发展及重要的方法**<br>

* 词：词嵌入，一种单个词的分布式表达，可以表示词的语义信息
* 可以很好获得短语和句子的包含语义的分布式表达的方法主要分为两类：
> 1. 通过无监督学习获得的全局句子嵌入表达：SkipThought vectors和DBOW模型。全局训练可以利用大量的不标记的语料。
> 2. 通过监督学习，为某个明确的任务训练得到的句子表达。通常与接下来的任务有关。
> 3. 通常，特定训练的句子表达性能优于全局表达。

* 接下来详细介绍在特定任务下，如何嵌入式表达句子。比如各种RNN和CNN
> 1. 利用注意力机制，在CNN或者LSTM模型上提取信息源来知道提取句子嵌入表达（没理解是什么鬼！！）
> 2. 在LSTM的每步结果上加max pooling / averaging
> 3. 直接利用最后一步的隐藏层作为句子的嵌入式表达

* **本文假设：**获取一个RNN模型的每一步语义表达是相对困难且不必要的。
> * 本文提出的自注意力机制可以将句子不同方面的语义提取至不同的向量表达
> * 本方法利用了LSTM的记忆（即中间输出）

## 方法
### 模型（自注意力机制）
**模型包括两个部分，一部分是biLSTM，另一部分是自注意力机制。biLSTM获得一般隐藏层表达，自注意力机制获得一个权重矩阵。两个矩阵做点积，句子的嵌入式表达（结果是一个矩阵）**<br>

* biLSTM
> * 每个句子包含n个字（或者词），每个词用d维的Embedding表示（word2vec/Glove/simple），则这个句子表示为一个2维矩阵。此时，句子中的每个词是独立的。<br>
> ![sentence](./imgs/sentence.png)
> * 用双向LSTM(biLSTM)获取单个句子中相邻词的依赖关系。隐藏层的维度是u。<br>
> ![biLSTM](./imgs/biLSTM.png)
> * 直接将两个LSTM获得的隐藏层拼接起来，得到n个2u维的向量。<br>
> ![H](./imgs/H.png)

* 自注意力机制
> * 目标：将在H中的n个结合LSTM隐藏层的向量进行线性组合，获得一个fixed_size（固定长度）的嵌入式表达。<br>
> ![weight_a](./imgs/weight_a.png)
> * 这个相当于两层MLP。第一层的输入是H^T（一个2u×n的矩阵），权重是Ws1（一个d×2u的矩阵）,激活函数是tanh，输出是一个d×n的矩阵;第二层的输入为第一层的输出，权重是ws2（一个d维向量），激活函数是softmax，输出是自注意机制部分的权重a（一个n维向量）。
> * 将获得的权重a乘以H得到的向量m（一个2u维的向量）作为句子的表达。
> * 上面的步骤，只能表达句子语义的一个方面或者内容。（这个有点牵强），如果需要从多个方面表示句子的语义信息，则要多个自注意力权重。提取一个句子的r个不同的方面。将ws2扩展为Ws2（一个r×d的矩阵），得到的自注意力权重从a扩展为A（一个r×n的矩阵）。这里的softmax仅对输入的第二维作用。<br>
> ![weights_A](./imgs/weights_A.png)
> * 最后句子的嵌入式表达M（r×2u的矩阵）。<br>
> ![M](./imgs/M.png)

### 惩罚项
**问题：如果自注意力机制提供相似的权重，句子的嵌入式表达矩阵M就有冗余的问题。所以需要惩罚项，来保证注意力机制得到的不同的求和权重向量尽量多样化（diversity）。**<br>

* Kullback Leibler divergence：权重向量中的任意两个的KL距离是评价多样性的最好的方法。（不晓得这个距离是个什么鬼！据说计算成本非常大，所以不用这个方法）
* 评价冗余程度：见下图
> ![measure_redundancy](./imgs/measure_redundancy.png)<br>
> 其中，A是自注意力产生的权重矩阵，I是单位阵，之后求Frobenius范数。类似于L2正则项，惩罚项P将被一个系数相乘，然后之前的loss一起被最小化（这个loss取决于之后的应用）。
> 

### 可视化

## 相关工作

## 试验结果

## 结果和讨论
